{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM HW 3\n",
    "\n",
    "For this homework, no dataset has been provided. Instead, you have to build your own. Your search engine will run on text documents. So, here we detail the procedure to follow for the data collection. We strongly suggest you work on different modules when implementing the required functions. For example, you may have a crawler.py module, a parser.py module, and a engine.py module: this is a good practice that improves readability in reporting and efficiency in deploying the code. Be careful; you are likely dealing with exceptions and other possible issues!\n",
    "\n",
    "### 1.1. Get the list of Michelin restaurants\n",
    "* You should begin by compiling a list of restaurants to include in your document corpus. Specifically, you will focus on web scraping the Michelin Restaurants in Italy. Your task is to collect the URL associated with each restaurant in this list. The output of this step should be a .txt file where each line contains a single restaurant’s URL. By the end, you should have approximately 2,000 restaurants on your list. The number changes daily, so some groups might have different number of restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We will start by loading the relevant libraries first. Then we will try to scrape the *relevant* links on only the first page of the [Michelin website](https://guide.michelin.com/en/it/restaurants/) to test. If the operation goes on successfully, we will scrape the links from the 100 pages!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import lxml\n",
    "import csv\n",
    "import random   \n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession, ClientResponseError\n",
    "from crawler import *\n",
    "from myparser import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting current working directory to /Users/saifdev/Desktop/ADM/ADM_HW3\n",
    "os.chdir(\"/Users/saifdev/Desktop/ADM/ADM_HW3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I begin to build the function `scraping_urls`, which will scrape all the links for the Michelin restaurants in Italy and save them to `urls.txt`, I will start by trying to scrape only the first page to test and practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SMALL TEST\n",
    "url = \"https://guide.michelin.com/en/it/restaurants/page/\" # url of the Michelin Guide Italy\n",
    "with open('test.txt', 'w') as file: # open a file to write the links of the restaurants\n",
    "    current_page = url + \"1\" # the first page only for testing\n",
    "    request = requests.get(current_page) # get the first page\n",
    "    soup = BeautifulSoup(request.content, 'lxml') # parse the content of the first page\n",
    "    for a in soup.select(\"div.flex-fill a\"): # Ref Keith Galli Y.T. channel :)\n",
    "        file.write(\"https://guide.michelin.com\"+a.get('href') + '\\n')   # write the links of the restaurants in the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of urls scraped is 20\n"
     ]
    }
   ],
   "source": [
    "# check the number of lines written in the file = 20 (and also manually checked)\n",
    "with open('test.txt', 'r') as file: # open the file to read the links of the restaurants\n",
    "    num_lines = len(file.readlines()) # count the number of lines in the file\n",
    "    result = \"20\" if num_lines == 20 else \"not 20\" # check if the number of lines is 20\n",
    "    print(f\"The number of urls scraped is {result}\") # print the result. It is indeed 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked as expected. Now we will scrape the urls from the whole 100 pages on the Micheline website using the `scraping_urls` function.\n",
    "\n",
    "A little note about the `scraping_urls` function:\n",
    "* It is a function that takes the url of the Michelin Guide Italy webpage and the number of pages to scrape as input and returns a text file containing the URLs of all the restaurants. The function is defined in the crawler.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Pages: 100%|██████████| 100/100 [03:37<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "pages = 100 # set the number of pages to scrape\n",
    "url = \"https://guide.michelin.com/en/it/restaurants/page/\" # url of the Michelin Guide Italy\n",
    "scraping_urls(url=url, pages=pages) # scrape the urls of the restaurants in the Michelin Guide Italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the number of urls scraped is approximately 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of restaurants urls scraped is 2029\n"
     ]
    }
   ],
   "source": [
    "# check the number of urls scraped\n",
    "with open('urls.txt', 'r') as file:\n",
    "    num_lines = len(file.readlines())\n",
    "    print(f\"Total Number of restaurants urls scraped is {num_lines}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2. Crawl Michelin restaurant pages\n",
    "\n",
    "##### Once you have all the URLs on the list, you should:\n",
    "\n",
    "* Download the HTML corresponding to each of the collected URLs.\n",
    "* After collecting each page, immediately save its HTML in a file. This way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "* Organize the downloaded HTML pages into folders. Each folder will contain the HTML of the restaurants from page 1, page 2, ... of the Michelin restaurant list.\n",
    "* Tip: Due to the large number of pages to download, consider using methods that can help shorten the process. If you employed a particular process or approach, kindly describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will scrape the information of the restaurants from the urls scraped. To do this, we will use the function load_urls, fetch_and_save, and download_html_in_batches. All these functions are defined in the crawler.py file.\n",
    "\n",
    "Some information about each of these functions:\n",
    "* **load_urls**: This function loads the urls of the restaurants from the urls.txt file.\n",
    "* **fetch_and_save**: This function fetches the html content of the urls and saves the html file into a folder of a particular batch/page.\n",
    "* **download_html_in_batches**: This function downloads the html content of the urls in batches of 20. The function makes used of `asyncio` and `aiohttp` to download the html content in parallel. This function is used to speed up the process of downloading the html content of the urls due to concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_file = 'urls.txt' # file containing the urls of the restaurants\n",
    "output_dir = 'michelin_html_batches' # directory to save the sub-directories and the html files\n",
    "urls = load_urls(url_file) # load the urls of the restaurants\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    await download_html_in_batches(urls, output_dir) # using asyncio environment to download the html files in batches\n",
    "except RuntimeError:\n",
    "    # Check if there's already an event loop running. Why? Because we can't create a new event loop in a thread that already has\n",
    "    # an event loop running. Jupyter notebooks often have an event loop running by default. If we try to create a new event loop\n",
    "    # in a Jupyter notebook, we'll get a RuntimeError. Thus, use asyncio.run if not in a running event loop environment\n",
    "    asyncio.run(download_html_in_batches(urls, output_dir))\n",
    "\n",
    "print(f\"Finished downloading in {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the html files of the restaurants in the Michelin Guide Italy website downloaded in batches - organised into folders. Now is the time that we start parsing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "##### At this point, you should have all the HTML documents about the restaurant of interest, and you can start to extract specific information. The list of the information we desire for each restaurant and their format is as follows:\n",
    "\n",
    "* Restaurant Name (to save as restaurantName): string;\n",
    "* Address (to save as address): string;\n",
    "* City (to save as city): string;\n",
    "* Postal Code (to save as postalCode): string;\n",
    "* Country (to save as country): string;\n",
    "* Price Range (to save as priceRange): string;\n",
    "* Cuisine Type (to save as cuisineType): string;\n",
    "* Description (to save as description): string;\n",
    "* Facilities and Services (to save as facilitiesServices): list of strings;\n",
    "* Accepted Credit Cards (to save as creditCards): list of strings;\n",
    "* Phone Number (to save as phoneNumber): string;\n",
    "* URL to the Restaurant Page (to save as website): string. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have created many functions to parse the html files and extract the information we need. These functions ae in the myparser.py file. Here is the list of functions. The functions are listed below and their names are self-explanatory:  \n",
    "\n",
    "* `get_restaurant_name(soup)` - taking in soup object and returning the name of the restaurant for example.\n",
    "* `get_address(soup)`\n",
    "* `get_city(address)`\n",
    "* `get_postal_code(address)`\n",
    "* `get_country(address)`\n",
    "* `get_price_range(soup)`\n",
    "* `get_cuisine_type(soup)`\n",
    "* `get_description(soup)`\n",
    "* `get_facilities_services(soup)`\n",
    "* `get_credit_cards(soup)`\n",
    "* `get_phone_number(soup)`\n",
    "* `get_website(soup)`\n",
    "\n",
    "We will test these functions of a random html file form the directories we have created. This will help us to understand the output of these functions and help us evaluate how well they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant name: 20Tre\n",
      "Restaurant address: via David Chiossone 20 r, Genoa, 16123, Italy\n",
      "Restaurant city: Genoa\n",
      "Restaurant zipcode: 16123\n",
      "Restaurant country: Italy\n",
      "Restaurant price range: €€\n",
      "Restaurant cuisine type: Farm to table, Modern Cuisine\n",
      "Restaurant description: Situated in the heart of Genoa’s historic centre, this contemporary-style restaurant focuses on just a few dishes, almost all fish-based, presented in a very modern style and in generous portions. Seasonal ingredients and market-fresh produce are the guiding philosophy here.\n",
      "Restaurant facilities and services: ['Air conditioning']\n",
      "Restaurant credit cards: ['amex', 'dinersclub', 'mastercard', 'visa']\n",
      "Restaurant phone number: +39 010 247 6191\n",
      "Restaurant website: https://www.ristorante20tregenova.it/\n"
     ]
    }
   ],
   "source": [
    "# Test the parser functions on a particular html file\n",
    "with open('/Users/saifdev/Desktop/ADM/ADM_HW3/michelin_html_batches/batch_1/20tre.html', 'r', encoding='utf-8') as file:\n",
    "\tcontent = file.read()\n",
    "\tsoup = BeautifulSoup(content, 'lxml')\n",
    "\tname = get_restaurant_name(soup)\n",
    "\tprint(f\"Restaurant name: {name}\")\n",
    "\taddress = get_address(soup)\n",
    "\tprint(f\"Restaurant address: {address}\")\n",
    "\tcity = get_city(address)\n",
    "\tprint(f\"Restaurant city: {city}\")\n",
    "\tzipcode = get_postal_code(address)\n",
    "\tprint(f\"Restaurant zipcode: {zipcode}\")\n",
    "\tcountry = get_country(address)\n",
    "\tprint(f\"Restaurant country: {country}\")\n",
    "\tprice = get_price_range(soup)\n",
    "\tprint(f\"Restaurant price range: {price}\")\n",
    "\tcuisine = get_cuisine_type(soup)\n",
    "\tprint(f\"Restaurant cuisine type: {cuisine}\")\n",
    "\tdescription = get_description(soup)\n",
    "\tprint(f\"Restaurant description: {description}\")\n",
    "\tfacilities = get_facilities_services(soup)\n",
    "\tprint(f\"Restaurant facilities and services: {facilities}\")\n",
    "\tcredit_cards = get_credit_cards(soup)\n",
    "\tprint(f\"Restaurant credit cards: {credit_cards}\")\n",
    "\tphone = get_phone_number(soup)\n",
    "\tprint(f\"Restaurant phone number: {phone}\")\n",
    "\twebsite = get_website(soup)\n",
    "\tprint(f\"Restaurant website: {website}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Parser functions work perfectly. Let's employ them now to get the required data from all the html files we have stored. Then, we will gather the data into a csv file which we can later read into the Pandas DataFrame.\n",
    "\n",
    "To do this, we will use the following functions stored in myparser.py:\n",
    "`extract_restaurant_data`, `save_restaurant_data_to_csv`, `forming_a_csv`\n",
    "\n",
    "A brief explanation of the functions:\n",
    "* `extract_restaurant_data` - This function takes the html content of a restaurant page and extracts the relevant data using the parser functions.\n",
    "* `save_restaurant_data_to_csv` - This function takes the extracted data and saves it to a csv file.\n",
    "* `forming_a_csv` - This function takes the html files stored in the directory and extracts the data from each file and saves it to a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to michelin_restaurant_data.csv\n"
     ]
    }
   ],
   "source": [
    "# form a csv file with the information of the restaurants using extract_restaurant_data function and the save_restaurant_data_to_csv function\n",
    "forming_a_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have the csv file with the information of the restaurants using the aforementioned functions. The csv file is named *\"michelin_restaurants.csv\"*. We can now load the csv file using `pd.read_csv` and perform the required tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurantName</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>country</th>\n",
       "      <th>priceRange</th>\n",
       "      <th>cuisineType</th>\n",
       "      <th>description</th>\n",
       "      <th>facilitiesServices</th>\n",
       "      <th>creditCards</th>\n",
       "      <th>phoneNumber</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Locanda Radici</td>\n",
       "      <td>SP 21, contrada San Vincenzo, Melizzano, 82030...</td>\n",
       "      <td>Melizzano</td>\n",
       "      <td>82030.0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>€€</td>\n",
       "      <td>Modern Cuisine, Campanian</td>\n",
       "      <td>A rustic restaurant with contemporary Mediterr...</td>\n",
       "      <td>Air conditioning; Car park; Garden or park; Gr...</td>\n",
       "      <td>amex; mastercard; visa</td>\n",
       "      <td>+39 0824 944506</td>\n",
       "      <td>https://www.locandaradici.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Posta</td>\n",
       "      <td>viale Vittorio Veneto 169, Sant'Omobono Terme,...</td>\n",
       "      <td>Sant'Omobono Terme</td>\n",
       "      <td>24038.0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>€€€</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Situated in the Imagna valley, this welcoming,...</td>\n",
       "      <td>Air conditioning; Wheelchair access</td>\n",
       "      <td>amex; dinersclub; mastercard; visa</td>\n",
       "      <td>+39 035 851134</td>\n",
       "      <td>https://www.frosioristoranti.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hostaria Baccofurore</td>\n",
       "      <td>via G.B. Lama 9, Furore, 84010, Italy</td>\n",
       "      <td>Furore</td>\n",
       "      <td>84010.0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>€€</td>\n",
       "      <td>Regional Cuisine, Farm to table</td>\n",
       "      <td>Patience is needed to get to this restaurant f...</td>\n",
       "      <td>Air conditioning; Car park; Garden or park; Gr...</td>\n",
       "      <td>amex; dinersclub; jcb; maestrocard; mastercard...</td>\n",
       "      <td>+39 089 830360</td>\n",
       "      <td>https://www.baccofurore.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nni Lausta</td>\n",
       "      <td>via Risorgimento 188, Santa Marina Salina, 980...</td>\n",
       "      <td>Santa Marina Salina</td>\n",
       "      <td>98050.0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>€€</td>\n",
       "      <td>Seafood, Seasonal Cuisine</td>\n",
       "      <td>Fish plays the starring role here, where the a...</td>\n",
       "      <td>Terrace</td>\n",
       "      <td>amex; jcb; maestrocard; mastercard; visa</td>\n",
       "      <td>+39 090 984 3486</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Osteria de Börg</td>\n",
       "      <td>via Forzieri 12, Rimini, 47921, Italy</td>\n",
       "      <td>Rimini</td>\n",
       "      <td>47921.0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>€</td>\n",
       "      <td>Cuisine from Romagna, Traditional Cuisine</td>\n",
       "      <td>Borgo San Giuliano is situated just a stroll f...</td>\n",
       "      <td>Terrace</td>\n",
       "      <td>amex; maestrocard; mastercard; visa</td>\n",
       "      <td>+39 0541 56074</td>\n",
       "      <td>https://www.osteriadeborg.it/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         restaurantName                                            address  \\\n",
       "0        Locanda Radici  SP 21, contrada San Vincenzo, Melizzano, 82030...   \n",
       "1                 Posta  viale Vittorio Veneto 169, Sant'Omobono Terme,...   \n",
       "2  Hostaria Baccofurore              via G.B. Lama 9, Furore, 84010, Italy   \n",
       "3            Nni Lausta  via Risorgimento 188, Santa Marina Salina, 980...   \n",
       "4       Osteria de Börg              via Forzieri 12, Rimini, 47921, Italy   \n",
       "\n",
       "                  city  postalCode country priceRange  \\\n",
       "0            Melizzano     82030.0   Italy         €€   \n",
       "1   Sant'Omobono Terme     24038.0   Italy        €€€   \n",
       "2               Furore     84010.0   Italy         €€   \n",
       "3  Santa Marina Salina     98050.0   Italy         €€   \n",
       "4               Rimini     47921.0   Italy          €   \n",
       "\n",
       "                                 cuisineType  \\\n",
       "0                  Modern Cuisine, Campanian   \n",
       "1                                    Italian   \n",
       "2            Regional Cuisine, Farm to table   \n",
       "3                  Seafood, Seasonal Cuisine   \n",
       "4  Cuisine from Romagna, Traditional Cuisine   \n",
       "\n",
       "                                         description  \\\n",
       "0  A rustic restaurant with contemporary Mediterr...   \n",
       "1  Situated in the Imagna valley, this welcoming,...   \n",
       "2  Patience is needed to get to this restaurant f...   \n",
       "3  Fish plays the starring role here, where the a...   \n",
       "4  Borgo San Giuliano is situated just a stroll f...   \n",
       "\n",
       "                                  facilitiesServices  \\\n",
       "0  Air conditioning; Car park; Garden or park; Gr...   \n",
       "1                Air conditioning; Wheelchair access   \n",
       "2  Air conditioning; Car park; Garden or park; Gr...   \n",
       "3                                            Terrace   \n",
       "4                                            Terrace   \n",
       "\n",
       "                                         creditCards       phoneNumber  \\\n",
       "0                             amex; mastercard; visa   +39 0824 944506   \n",
       "1                 amex; dinersclub; mastercard; visa    +39 035 851134   \n",
       "2  amex; dinersclub; jcb; maestrocard; mastercard...    +39 089 830360   \n",
       "3           amex; jcb; maestrocard; mastercard; visa  +39 090 984 3486   \n",
       "4                amex; maestrocard; mastercard; visa    +39 0541 56074   \n",
       "\n",
       "                           website  \n",
       "0    https://www.locandaradici.it/  \n",
       "1  https://www.frosioristoranti.it  \n",
       "2      https://www.baccofurore.it/  \n",
       "3                              NaN  \n",
       "4    https://www.osteriadeborg.it/  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('michelin_restaurant_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the shape of the data: It should be (2029, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Checking the shape of the data: It should be {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2029 entries, 0 to 2028\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   restaurantName      2029 non-null   object \n",
      " 1   address             2029 non-null   object \n",
      " 2   city                2000 non-null   object \n",
      " 3   postalCode          1983 non-null   float64\n",
      " 4   country             2029 non-null   object \n",
      " 5   priceRange          1983 non-null   object \n",
      " 6   cuisineType         2029 non-null   object \n",
      " 7   description         2029 non-null   object \n",
      " 8   facilitiesServices  1959 non-null   object \n",
      " 9   creditCards         1979 non-null   object \n",
      " 10  phoneNumber         1983 non-null   object \n",
      " 11  website             1875 non-null   object \n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 190.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Preprocessing\n",
    "## 2.0.0) Preprocessing the text\n",
    "\n",
    "Before building the search engine, you must clean and prepare the text in each restaurant’s description.\n",
    "\n",
    "We will:\n",
    "\n",
    "+ Remove stopwords.\n",
    "+ Remove punctuation.\n",
    "+ Apply stemming.\n",
    "+ Perform any other necessary cleaning to improve search accuracy.\n",
    ">For this, we use the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\val\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 41.5/41.5 kB 496.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\val\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\val\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.6/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 273.6/273.6 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.5 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.9.1 regex-2024.11.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/Val/AppData/Roaming/nltk_data']\n",
      "C:\\Users\\Val\\AppData\\Roaming\\nltk_data\\corpora\\stopwords.zip\n",
      "C:\\Users\\Val\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Val\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Val\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "import nltk\n",
    "nltk.data.path = ['C:/Users/Val/AppData/Roaming/nltk_data']\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(nltk.data.path)\n",
    "\n",
    "print(nltk.data.find('corpora/stopwords.zip'))\n",
    "print(nltk.data.find('tokenizers/punkt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(string):\n",
    "\n",
    "    #ensure text is a string (in case of NaN or other data types)\n",
    "    if not isinstance(string, str):\n",
    "        return \"\"\n",
    "    \n",
    "    #to lower case\n",
    "    string = string.lower()\n",
    "    # Remove extra whitespace\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "    #handling compound words with '-' to treat them as separate tokens\n",
    "    string = re.sub(r'-', ' ', string) \n",
    "\n",
    "    #tokenize\n",
    "    tokens = word_tokenize(string)\n",
    "    #remove punctuation: keep only aplhanumeric tokens\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    #remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    #lemmitization before stemming\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    #stemming to reduce words to their root in order to reduce our vocabulary size, improve search accuracy and speed up search process\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    #return the cleaned string\n",
    "    preprocessed_string = ' '.join(tokens)\n",
    "    return preprocessed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:/Users/Val/AppData/Roaming/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_description\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[46], line 19\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     16\u001b[0m string \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, string) \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#tokenize\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#remove punctuation: keep only aplhanumeric tokens\u001b[39;00m\n\u001b[0;32m     21\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalnum()]\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\Val\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:/Users/Val/AppData/Roaming/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "data['cleaned_description'] = data['description'].apply(preprocessing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
